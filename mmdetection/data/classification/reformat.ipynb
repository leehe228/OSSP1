{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image = \"./old_train/img/\"\n",
    "train_ann = \"./old_train/ann/\"\n",
    "\n",
    "val_image = \"./old_valid/img/\"\n",
    "val_ann = \"./old_valid/ann/\"\n",
    "\n",
    "test_image = \"./old_test/img/\"\n",
    "test_ann = \"./old_test/ann/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  1991 1991\n",
      "VAL:  254 254\n",
      "TEST:  250 250\n"
     ]
    }
   ],
   "source": [
    "train_image_list = os.listdir(train_image)\n",
    "train_ann_list = os.listdir(train_ann)\n",
    "\n",
    "val_image_list = os.listdir(val_image)\n",
    "val_ann_list = os.listdir(val_ann)\n",
    "\n",
    "test_image_list = os.listdir(test_image)\n",
    "test_ann_list = os.listdir(test_ann)\n",
    "\n",
    "print(\"TRAIN: \", len(train_image_list), len(train_ann_list))\n",
    "print(\"VAL: \", len(val_image_list), len(val_ann_list))\n",
    "print(\"TEST: \", len(test_image_list), len(test_ann_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 254/254 [00:00<00:00, 94925.89it/s]\n",
      "100%|██████████| 254/254 [00:00<00:00, 138880.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# rename image and ann\n",
    "# 3449_jpg.rf.f0cbd3e5b418f05bfdab2e4bf556086e.jpg -> 3349.jpg\n",
    "\n",
    "for i in tqdm(val_image_list):\n",
    "    os.rename(val_image+i, val_image+i.split(\"_\")[0]+\".jpg\")\n",
    "    \n",
    "# rename ann json\n",
    "# 1_jpg.rf.a332191a2b0b318c103a508c515931c1.jpg.json -> 1.json\n",
    "for i in tqdm(val_ann_list):\n",
    "    os.rename(val_ann+i, val_ann+i.split(\"_\")[0]+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1991/1991 [00:00<00:00, 92071.22it/s]\n",
      "100%|██████████| 1991/1991 [00:00<00:00, 99776.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# rename image and ann\n",
    "# 1.jpg -> 10001.jpg, 1.json -> 10001.json\n",
    "for i in tqdm(train_image_list):\n",
    "    os.rename(train_image+i, train_image+str(int(i.split(\"_\")[0])+10000)+\".jpg\")\n",
    "    \n",
    "for i in tqdm(train_ann_list):\n",
    "    os.rename(train_ann+i, train_ann+str(int(i.split(\"_\")[0])+10000)+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 62541.81it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(test_image_list):\n",
    "    os.rename(test_image+i, test_image+str(int(i.split(\"_\")[0])+20000)+\".jpg\")\n",
    "\n",
    "for i in tqdm(test_ann_list):\n",
    "    os.rename(test_ann+i, test_ann+str(int(i.split(\"_\")[0])+20000)+\".json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  1991 1991\n",
      "VAL:  254 254\n",
      "TEST:  250 250\n"
     ]
    }
   ],
   "source": [
    "new_train_image_list = os.listdir(train_image)\n",
    "new_train_ann_list = os.listdir(train_ann)\n",
    "new_val_image_list = os.listdir(val_image)\n",
    "new_val_ann_list = os.listdir(val_ann)\n",
    "new_test_image_list = os.listdir(test_image)\n",
    "new_test_ann_list = os.listdir(test_ann)\n",
    "\n",
    "print(\"TRAIN: \", len(new_train_image_list), len(new_train_ann_list))\n",
    "print(\"VAL: \", len(new_val_image_list), len(new_val_ann_list))\n",
    "print(\"TEST: \", len(new_test_image_list), len(new_test_ann_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in new_train_image_list:\n",
    "    if i.split(\".\")[0] + \".json\" not in new_train_ann_list:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in new_test_image_list:\n",
    "    if i.split(\".\")[0] + \".json\" not in new_test_ann_list:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in new_val_image_list:\n",
    "    if i.split(\".\")[0] + \".json\" not in new_val_ann_list:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_to_coco(data_dir, output_file):\n",
    "    images = []\n",
    "    annotations = []\n",
    "    categories = [{\"id\": 0, \"name\": \"tooth\", \"supercategory\": \"none\"}, {\"id\":1, \"name\": \"cavity\", \"supercategory\": \"none\"}]\n",
    "    \n",
    "    # category_id = 1  # Since all categories are \"Tooth\"\n",
    "    annotation_id = 1\n",
    "    \n",
    "    img_dir = os.path.join(data_dir, 'img')\n",
    "    ann_dir = os.path.join(data_dir, 'ann')\n",
    "    \n",
    "    img_files = sorted(glob.glob(os.path.join(img_dir, '*.jpg')))\n",
    "    \n",
    "    for img_id, img_file in enumerate(tqdm(img_files), 1):\n",
    "        img_name = os.path.basename(img_file)\n",
    "        ann_file = os.path.join(ann_dir, os.path.splitext(img_name)[0] + '.json')\n",
    "\n",
    "        if not os.path.exists(ann_file):\n",
    "            continue\n",
    "        \n",
    "        with open(ann_file, 'r') as f:\n",
    "            ann_data = json.load(f)\n",
    "        \n",
    "        # Add image information\n",
    "        images.append({\n",
    "            \"id\": img_id,\n",
    "            \"file_name\": img_name,\n",
    "            \"height\": ann_data['size']['height'],\n",
    "            \"width\": ann_data['size']['width']\n",
    "        })\n",
    "        \n",
    "        # Add annotations\n",
    "        for obj in ann_data['objects']:\n",
    "            points = obj['points']['exterior']\n",
    "            # Convert polygon to COCO format (x, y, width, height)\n",
    "            x_coords = [p[0] for p in points]\n",
    "            y_coords = [p[1] for p in points]\n",
    "            min_x, min_y = min(x_coords), min(y_coords)\n",
    "            max_x, max_y = max(x_coords), max(y_coords)\n",
    "            width, height = max_x - min_x, max_y - min_y\n",
    "            classTitle = obj['classTitle']\n",
    "            \n",
    "            category_id = 0 if classTitle == 'Tooth' else 1\n",
    "            \n",
    "            annotations.append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"segmentation\": [list(sum(points, []))],  # Flatten the list of points\n",
    "                \"area\": width * height,\n",
    "                \"bbox\": [min_x, min_y, width, height],\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            annotation_id += 1\n",
    "    \n",
    "    coco_format = {\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "        \"categories\": categories\n",
    "    }\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(coco_format, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1991/1991 [00:00<00:00, 3424.03it/s]\n"
     ]
    }
   ],
   "source": [
    "convert_to_coco('./old_train/', 'train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 254/254 [00:00<00:00, 4006.73it/s]\n"
     ]
    }
   ],
   "source": [
    "convert_to_coco(\"./old_valid/\", \"val.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 4271.34it/s]\n"
     ]
    }
   ],
   "source": [
    "convert_to_coco(\"./old_test/\", \"test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import easydict\n",
    "from PIL import Image as im\n",
    "from PIL import ImageOps\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import sys\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "op = os.path.join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Directory Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-19_05_43\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"/home/dmsai2/mmdetection/data/\"\n",
    "\n",
    "TRAIN_DATASET_DIR = op(DATA_DIR, \"classification\")\n",
    "TRAIN_IMAGE_DIR = op(TRAIN_DATASET_DIR, \"train\")\n",
    "TRAIN_JSON_DIR = op(TRAIN_DATASET_DIR, \"annotations\")\n",
    "\n",
    "TEST_DATASET_DIR = op(DATA_DIR, \"classification\")\n",
    "TEST_IMAGE_DIR = op(TEST_DATASET_DIR, \"test\")\n",
    "TEST_JSON_DIR = op(TEST_DATASET_DIR, \"annotations\")\n",
    "\n",
    "current_datetime = datetime.now()\n",
    "formatted_datetime = current_datetime.strftime('%Y-%m-%d_%H_%M')\n",
    "print(formatted_datetime)\n",
    "\n",
    "os.makedirs(op(\"/home/dmsai2/mmdetection/work_dir/classification/logs\", formatted_datetime), exist_ok=True)\n",
    "os.makedirs(op(\"/home/dmsai2/mmdetection/work_dir/classification/weights\", formatted_datetime), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read File List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train file: 1991\n",
      "number of train file: 250\n"
     ]
    }
   ],
   "source": [
    "train_file_list = list(map(lambda x : x.split(\".\")[0], os.listdir(TRAIN_IMAGE_DIR)))\n",
    "train_image_list = os.listdir(TRAIN_IMAGE_DIR)\n",
    "train_json_list = os.listdir(TRAIN_JSON_DIR)\n",
    "print(\"number of train file:\", len(train_file_list))\n",
    "\n",
    "test_file_list = list(map(lambda x : x.split(\".\")[0], os.listdir(TEST_IMAGE_DIR)))\n",
    "test_image_list = os.listdir(TEST_IMAGE_DIR)\n",
    "test_json_list = os.listdir(TEST_JSON_DIR)\n",
    "print(\"number of train file:\", len(test_file_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\"/hoeunlee228/Dataset/train_df.csv\")\n",
    "# # print(train_df.head())\n",
    "# train_df['is_decayed'].value_counts()\n",
    "\n",
    "# not_decayed_rows = train_df[train_df['is_decayed'] == False]\n",
    "# decayed_rows = train_df[train_df['is_decayed'] == True]\n",
    "# print(\"total not decayed rows:\", len(not_decayed_rows), \"total decayed rows\", len(decayed_rows))\n",
    "\n",
    "# num_samples_not_decayed = 28136 * 3\n",
    "# num_samples_decayed = 28136\n",
    "\n",
    "# print(\"args num sampled not decayed:\", num_samples_not_decayed, \"args num sampled decayed:\", num_samples_decayed)\n",
    "\n",
    "# random_samples_not_decayed = random.sample(range(len(not_decayed_rows)), num_samples_not_decayed)\n",
    "# not_decayed_sampled_df = not_decayed_rows.iloc[random_samples_not_decayed]\n",
    "# print(\"num of not decayed sampled list\", len(not_decayed_sampled_df))\n",
    "\n",
    "# random_samples_decayed = random.sample(range(len(decayed_rows)), num_samples_decayed)\n",
    "# decayed_sampled_df = decayed_rows.iloc[random_samples_decayed]\n",
    "# print(\"num of decayed sampled list\", len(decayed_sampled_df))\n",
    "\n",
    "# train_file_list_not_decayed_sampled = list(map(lambda x : f\"{x[0]}_{x[1]}\", not_decayed_sampled_df[['file', 'teeth_idx']].values.tolist()))\n",
    "# train_file_list_decayed_sampled = list(map(lambda x : f\"{x[0]}_{x[1]}\", decayed_sampled_df[['file', 'teeth_idx']].values.tolist()))\n",
    "# print(\"num of sampled file list (not decayed, decayed):\", len(train_file_list_not_decayed_sampled), len(train_file_list_decayed_sampled))\n",
    "\n",
    "# sampled_train_list = train_file_list_not_decayed_sampled + train_file_list_decayed_sampled\n",
    "# print(\"total num of sampled train list:\", len(sampled_train_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ToothDataset(Dataset):\n",
    "#     def __init__(self, data_dir, ann_dir, file_list, transform=None, aug_transform=None, valid=False):\n",
    "#         self.data_dir = data_dir\n",
    "#         self.ann_dir = ann_dir\n",
    "#         self.file_list = file_list\n",
    "#         self.transform = transform\n",
    "#         self.aug_transform = aug_transform\n",
    "#         self.valid = valid\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.file_list)\n",
    "    \n",
    "#     def _load_image(self, image_path):\n",
    "#         assert os.path.exists(op(self.data_dir, image_path))\n",
    "#         # return cv2.cvtColor(cv2.imread(op(self.data_dir, \"image\", image_path)), cv2.COLOR_BGR2RGB)\n",
    "#         if self.valid:\n",
    "#             return (im.open(op(self.data_dir, image_path)).convert(\"RGB\"), image_path)\n",
    "#         else:\n",
    "#             return im.open(op(self.data_dir, image_path)).convert(\"RGB\")\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         image_path = self.file_list[index] + \".png\"\n",
    "#         json_path = self.file_list[index] + \".json\"\n",
    "\n",
    "#         if self.valid:\n",
    "#             image, label = self._load_image(image_path)\n",
    "#         else:\n",
    "#             image = self._load_image(image_path)\n",
    "\n",
    "#         with open(op(self.ann_dir, json_path), 'r') as json_file:\n",
    "#             data = json.load(json_file)\n",
    "\n",
    "#         decayed = data[\"tooth\"][0][\"decayed\"]\n",
    "#         target = 1 if decayed else 0\n",
    "\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "\n",
    "#         if self.aug_transform:\n",
    "#             image = self.aug_transform(image)\n",
    "\n",
    "#         if self.valid:\n",
    "#             return (image, label), target\n",
    "#         else:\n",
    "#             return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToothCOCODataset(Dataset):\n",
    "    def __init__(self, data_dir, ann_file, transform=None, aug_transform=None, valid=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.image_ids = self.coco.getImgIds()\n",
    "        self.transform = transform\n",
    "        self.aug_transform = aug_transform\n",
    "        self.valid = valid\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def _load_image(self, image_id):\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        image_path = op(self.data_dir, image_info['file_name'])\n",
    "        assert os.path.exists(image_path), f\"Image path {image_path} does not exist.\"\n",
    "        \n",
    "        if self.valid:\n",
    "            return (im.open(image_path).convert(\"RGB\"), image_info['file_name'])\n",
    "        else:\n",
    "            return im.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    def _load_target(self, image_id):\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        # Assuming 'decayed' is the attribute that indicates decay\n",
    "        # decayed = any(ann.get('category_id', False) for ann in anns)\n",
    "        decayed = any(ann.get('category_id', False) for ann in anns)\n",
    "        target = 1 if decayed else 0\n",
    "        return target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_id = self.image_ids[index]\n",
    "\n",
    "        if self.valid:\n",
    "            image, label = self._load_image(image_id)\n",
    "        else:\n",
    "            image = self._load_image(image_id)\n",
    "\n",
    "        target = self._load_target(image_id)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.aug_transform:\n",
    "            image = self.aug_transform(image)\n",
    "\n",
    "        if self.valid:\n",
    "            return (image, label), target\n",
    "        else:\n",
    "            return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n",
    "        super(SeparableConv2d,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n",
    "        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,in_filters,out_filters,reps,strides=1,start_with_relu=True,grow_first=True):\n",
    "        super(Block, self).__init__()\n",
    "\n",
    "        if out_filters != in_filters or strides!=1:\n",
    "            self.skip = nn.Conv2d(in_filters,out_filters,1,stride=strides, bias=False)\n",
    "            self.skipbn = nn.BatchNorm2d(out_filters)\n",
    "        else:\n",
    "            self.skip=None\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        rep=[]\n",
    "\n",
    "        filters=in_filters\n",
    "        if grow_first:\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n",
    "            rep.append(nn.BatchNorm2d(out_filters))\n",
    "            filters = out_filters\n",
    "\n",
    "        for i in range(reps-1):\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d(filters,filters,3,stride=1,padding=1,bias=False))\n",
    "            rep.append(nn.BatchNorm2d(filters))\n",
    "\n",
    "        if not grow_first:\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n",
    "            rep.append(nn.BatchNorm2d(out_filters))\n",
    "\n",
    "        if not start_with_relu:\n",
    "            rep = rep[1:]\n",
    "        else:\n",
    "            rep[0] = nn.ReLU(inplace=False)\n",
    "\n",
    "        if strides != 1:\n",
    "            rep.append(nn.MaxPool2d(3,strides,1))\n",
    "        self.rep = nn.Sequential(*rep)\n",
    "\n",
    "    def forward(self,inp):\n",
    "        x = self.rep(inp)\n",
    "\n",
    "        if self.skip is not None:\n",
    "            skip = self.skip(inp)\n",
    "            skip = self.skipbn(skip)\n",
    "        else:\n",
    "            skip = inp\n",
    "\n",
    "        x+=skip\n",
    "        return x\n",
    "\n",
    "\n",
    "class Xception(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(Xception, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3,32,3,2,0,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32,64,3,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.block1=Block(64,128,2,2,start_with_relu=False,grow_first=True)\n",
    "        self.block2=Block(128,256,2,2,start_with_relu=True,grow_first=True)\n",
    "        self.block3=Block(256,728,2,2,start_with_relu=True,grow_first=True)\n",
    "\n",
    "        self.block4=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block5=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block6=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block7=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "\n",
    "        self.block8=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block9=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block10=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block11=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "\n",
    "        self.block12=Block(728,1024,2,2,start_with_relu=True,grow_first=False)\n",
    "\n",
    "        self.conv3 = SeparableConv2d(1024,1536,3,1,1)\n",
    "        self.bn3 = nn.BatchNorm2d(1536)\n",
    "\n",
    "        self.conv4 = SeparableConv2d(1536,2048,3,1,1)\n",
    "        self.bn4 = nn.BatchNorm2d(2048)\n",
    "\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def features(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.block7(x)\n",
    "        x = self.block8(x)\n",
    "        x = self.block9(x)\n",
    "        x = self.block10(x)\n",
    "        x = self.block11(x)\n",
    "        x = self.block12(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        return x\n",
    "\n",
    "    def logits(self, features):\n",
    "        x = self.relu(features)\n",
    "\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1)) \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.last_linear(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.features(input)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "## 기존 Xception에 Dropout만 추가\n",
    "class xception(nn.Module):\n",
    "    def __init__(self, num_out_classes=2, dropout=0.5):\n",
    "        super(xception, self).__init__()\n",
    "\n",
    "        self.model = Xception(num_classes=num_out_classes)\n",
    "        self.model.last_linear = self.model.fc\n",
    "        del self.model.fc\n",
    "\n",
    "        num_ftrs = self.model.last_linear.in_features\n",
    "        if not dropout:\n",
    "            self.model.last_linear = nn.Linear(num_ftrs, num_out_classes)\n",
    "        else:            \n",
    "            self.model.last_linear = nn.Sequential(\n",
    "                nn.Dropout(p=dropout),\n",
    "                nn.Linear(num_ftrs, num_out_classes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"gpu\": 0,\n",
    "    \"num_workers\": 4,\n",
    "    \"root\": \"/home/dmsai2/mmdetection/work_dir/classification/\",\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"num_epochs\": 50,\n",
    "    \"batch_size\": 16,\n",
    "\n",
    "    \"save_fn\": f\"/home/dmsai2/mmdetection/work_dir/classification/weights/{formatted_datetime}/xception\",\n",
    "    \"load_fn\": None,\n",
    "    \"scheduler\": None,\n",
    "\n",
    "    \"scheduler_step\": 1,\n",
    "    \"scheduler_gamma\": 0.001\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Zero padding to make the image square\n",
    "def make_square(img):\n",
    "    width, height = img.size\n",
    "    max_side = max(width, height)\n",
    "    left = (max_side - width) // 2\n",
    "    top = (max_side - height) // 2\n",
    "    right = (max_side - width) - left\n",
    "    bottom = (max_side - height) - top\n",
    "    padding = (left, top, right, bottom)\n",
    "    return ImageOps.expand(img, padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = (0.57933619, 0.42688786, 0.33401168)\n",
    "std = (0.35580848, 0.27125023, 0.22251333)\n",
    "\n",
    "\"\"\"train_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\"\"\"\n",
    "\n",
    "train_aug_transform = transforms.Compose([\n",
    "    # 1. 이미지를 정사각형으로 만들기\n",
    "    transforms.Lambda(lambda img: ImageOps.exif_transpose(img)),  # Exif 정보 처리\n",
    "    transforms.Lambda(make_square),\n",
    "\n",
    "    # 2. Resize\n",
    "    transforms.Resize(size=(299, 299)),\n",
    "\n",
    "    # 3. Augmentation\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1),\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "test_aug_transform = transforms.Compose([\n",
    "    # 1. 이미지를 정사각형으로 만들기\n",
    "    transforms.Lambda(lambda img: ImageOps.exif_transpose(img)),  # Exif 정보 처리\n",
    "    transforms.Lambda(make_square),\n",
    "\n",
    "    # 2. Resize\n",
    "    transforms.Resize(size=(299, 299)),\n",
    "    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "valid_aug_transform = transforms.Compose([\n",
    "    # 1. 이미지를 정사각형으로 만들기\n",
    "    transforms.Lambda(lambda img: ImageOps.exif_transpose(img)),  # Exif 정보 처리\n",
    "    transforms.Lambda(make_square),\n",
    "\n",
    "    # 2. Resize\n",
    "    transforms.Resize(size=(299, 299)),\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    print(\"pth file saved at \" + filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, f\"/home/dmsai2/mmdetection/work_dir/classification/weights/{formatted_datetime}/model_best.pth.tar\")\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, args):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = args.lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / validate\n",
    "def train(train_loader, model, criterion, optimizer, epoch, writer, step):   \n",
    "    n = 0\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "\n",
    "    error_count = {\n",
    "        \"precision\": 0,\n",
    "        \"recall\": 0,\n",
    "        \"auc\": 0\n",
    "    }\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    with tqdm(train_loader, total=len(train_loader), desc=\"Train\", file=sys.stdout) as iterator:\n",
    "        for images, target in iterator:\n",
    "            if args.gpu is not None:\n",
    "                images = images.cuda(args.gpu, non_blocking=True)\n",
    "                target = target.cuda(args.gpu, non_blocking=True)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "\n",
    "            loss = criterion(outputs, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            n += images.size(0)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += torch.sum(pred == target.data)\n",
    "\n",
    "            epoch_loss = running_loss / float(n)\n",
    "            epoch_acc = running_corrects / float(n)\n",
    "\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "\n",
    "            # Calculate F1-Score\n",
    "            f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "\n",
    "            # Calculate Precision and Recall\n",
    "            try:\n",
    "                precision = precision_score(all_targets, all_preds, average='weighted', zero_division=1)\n",
    "            except Exception as e:\n",
    "                precision = -1.0\n",
    "                error_count[\"precision\"] += 1\n",
    "\n",
    "            try:\n",
    "                recall = recall_score(all_targets, all_preds, average='weighted')\n",
    "            except Exception as e:\n",
    "                recall = -1.0\n",
    "                error_count[\"recall\"] += 1\n",
    "\n",
    "            # Calculate AUC\n",
    "            # fpr, tpr, _ = roc_curve(target.cpu(), outputs.data[:, 1].cpu())\n",
    "            # auc_value = roc_auc_score(target.cpu(), outputs.data[:, 1].cpu())\n",
    "            try:\n",
    "                # fpr, tpr, _ = roc_curve(all_targets, all_preds)\n",
    "                auc_value = roc_auc_score(all_targets, all_preds)\n",
    "            except Exception as e:\n",
    "                auc_value = -1.0\n",
    "                error_count[\"auc\"] += 1\n",
    "\n",
    "            log = 'loss - {:.4f}, acc - {:.4f}, F1 - {:.4f}, Precision - {:.4f}, Recall - {:.4f}, AUC - {:.4f}'.format(epoch_loss, epoch_acc, f1, precision, recall, auc_value)\n",
    "            iterator.set_postfix_str(log)\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                writer.add_scalar(\"Train/Step/Loss\", epoch_loss, step)\n",
    "                writer.add_scalar(\"Train/Step/Accuracy\", epoch_acc, step)\n",
    "                writer.add_scalar(\"Train/Step/F1-Score\", f1, step)\n",
    "                writer.add_scalar(\"Train/Step/Precision\", precision, step)\n",
    "                writer.add_scalar(\"Train/Step/Recall\", recall, step)\n",
    "                writer.add_scalar(\"Train/Step/AUC\", auc_value, step)\n",
    "\n",
    "            step += 1\n",
    "\n",
    "    writer.add_scalar(\"Train/Epoch/Loss\", epoch_loss, epoch)\n",
    "    writer.add_scalar(\"Train/Epoch/Accuracy\", epoch_acc, epoch)\n",
    "    writer.add_scalar(\"Train/Epoch/F1-Score\", f1, epoch)\n",
    "    writer.add_scalar(\"Train/Epoch/Precision\", precision, epoch)\n",
    "    writer.add_scalar(\"Train/Epoch/Recall\", recall, epoch)\n",
    "    writer.add_scalar(\"Train/Epoch/AUC\", auc_value, epoch)\n",
    "\n",
    "    print(error_count)\n",
    "\n",
    "    # scheduler.step()\n",
    "\n",
    "    return step\n",
    "\n",
    "\n",
    "def validate(test_loader, model, criterion, epoch, writer):\n",
    "\n",
    "    all_lables = []\n",
    "\n",
    "    n = 0\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "\n",
    "    error_count = {\n",
    "        \"precision\": 0,\n",
    "        \"recall\": 0,\n",
    "        \"auc\": 0\n",
    "    }\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with tqdm(test_loader, total=len(test_loader), desc=\"Valid\", file=sys.stdout) as iterator:\n",
    "        for images, target in iterator:\n",
    "\n",
    "            images, labels = images[0], images[1]\n",
    "\n",
    "            all_lables.extend(labels)\n",
    "\n",
    "            if args.gpu is not None:\n",
    "                images = images.cuda(args.gpu, non_blocking=True)\n",
    "                target = target.cuda(args.gpu, non_blocking=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(images)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            _, pred = torch.max(output.data, 1)\n",
    "\n",
    "            n += images.size(0)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += torch.sum(pred == target.data)\n",
    "\n",
    "            epoch_loss = running_loss / float(n)\n",
    "            epoch_acc = running_corrects / float(n)\n",
    "\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "\n",
    "            # Calculate F1-Score\n",
    "            f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "\n",
    "            try:\n",
    "                precision = precision_score(all_targets, all_preds, average='weighted', zero_division=1)\n",
    "            except Exception as e:\n",
    "                precision = -1.0\n",
    "                error_count[\"precision\"] += 1\n",
    "            \n",
    "            try:\n",
    "                recall = recall_score(all_targets, all_preds, average='weighted')\n",
    "            except Exception as e:\n",
    "                recall = -1.0\n",
    "                error_count[\"recall\"] += 1\n",
    "\n",
    "            # Calculate AUC\n",
    "            # fpr, tpr, _ = roc_curve(target.cpu(), output.data[:, 1].cpu())\n",
    "            # auc_value = roc_auc_score(target.cpu(), output.data[:, 1].cpu())\n",
    "            try:\n",
    "                # fpr, tpr, _ = roc_curve(all_targets, all_preds)\n",
    "                auc_value = roc_auc_score(all_targets, all_preds)\n",
    "            except Exception as e:\n",
    "                auc_value = -1.0\n",
    "                error_count[\"auc\"] += 1\n",
    "\n",
    "            log = 'loss - {:.4f}, acc - {:.4f}, F1 - {:.4f}, Precision - {:.4f}, Recall - {:.4f}, AUC - {:.4f}'.format(epoch_loss, epoch_acc, f1, precision, recall, auc_value)\n",
    "            iterator.set_postfix_str(log)\n",
    "\n",
    "    writer.add_scalar(\"Validation/Epoch/Loss\", epoch_loss, epoch)\n",
    "    writer.add_scalar(\"Validation/Epoch/Accuracy\", epoch_acc, epoch)\n",
    "    writer.add_scalar(\"Validation/Epoch/F1-Score\", f1, epoch)\n",
    "    writer.add_scalar(\"Validation/Epoch/Precision\", precision, epoch)\n",
    "    writer.add_scalar(\"Validation/Epoch/Recall\", recall, epoch)\n",
    "    writer.add_scalar(\"Validation/Epoch/AUC\", auc_value, epoch)\n",
    "\n",
    "    print(error_count)\n",
    "\n",
    "    return epoch_acc, all_lables, all_preds, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorboardX를 사용하여 summary writer 생성\n",
    "train_writer = SummaryWriter(f\"/home/dmsai2/mmdetection/work_dir/classification/logs/{formatted_datetime}/train\")  # 'logs/train'는 로그가 저장될 디렉토리입니다.\n",
    "test_writer = SummaryWriter(f\"/home/dmsai2/mmdetection/work_dir/classification/logs/{formatted_datetime}/validation\")  # 'logs/validation'은 로그가 저장될 디렉토리입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating model 'xception'\n"
     ]
    }
   ],
   "source": [
    "model = xception(num_out_classes=2, dropout=0.5)\n",
    "print(\"=> creating model '{}'\".format('xception'))\n",
    "model = model.cuda(args.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.load_fn is not None:\n",
    "    assert os.path.isfile(args.load_fn), 'wrong path'\n",
    "\n",
    "    model.load_state_dict(torch.load(args.load_fn)['state_dict'])\n",
    "    print(\"=> model weight '{}' is loaded\".format(args.load_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, betas=(0.9, 0.999), eps=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.scheduler == \"steplr\":\n",
    "    print(\"StepLR selected to Scheduler\")\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, \n",
    "                                    step_size=args.scheduler_step,\n",
    "                                    gamma=args.scheduler_gamma)\n",
    "elif args.scheduler == 'exp':\n",
    "    print(\"ExponentialLR selected to Scheduler\")\n",
    "    scheduler = lr_scheduler.ExponentialLR(optimizer, \n",
    "                                    gamma=args.scheduler_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991 250\n",
      "loading annotations into memory...\n",
      "Done (t=0.29s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "train_file_list = list(map(lambda x : x.split(\".\")[0], os.listdir(\"/home/dmsai2/mmdetection/data/classification/train\")))\n",
    "test_file_list = list(map(lambda x : x.split(\".\")[0], os.listdir(\"/home/dmsai2/mmdetection/data/classification/test\")))\n",
    "print(len(train_file_list), len(test_file_list))\n",
    "\n",
    "# train_dataset = ToothCOCODataset(data_dir=\"/home/dmsai2/mmdetection/data/\",\n",
    "#                              file_list=train_file_list,\n",
    "#                              transform=train_aug_transform)\n",
    "\n",
    "# valid_dataset = ToothCOCODataset(data_dir=\"/home/dmsai2/mmdetection/data/\",\n",
    "#                              file_list=test_file_list,\n",
    "#                              transform=test_aug_transform, valid=True)\n",
    "\n",
    "train_dataset = ToothCOCODataset(data_dir=\"/home/dmsai2/mmdetection/data/classification/train\", \n",
    "                                 ann_file=\"/home/dmsai2/mmdetection/data/classification/annotations/train.json\", \n",
    "                                 aug_transform=train_aug_transform, valid=False)\n",
    "\n",
    "valid_dataset = ToothCOCODataset(data_dir=\"/home/dmsai2/mmdetection/data/classification/test\", \n",
    "                                 ann_file=\"/home/dmsai2/mmdetection/data/classification/annotations/test.json\", \n",
    "                                 aug_transform=train_aug_transform, valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=args.num_workers,\n",
    "                                           pin_memory=True,\n",
    "                                           )\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           shuffle=False,\n",
    "                                           num_workers=args.num_workers,\n",
    "                                           pin_memory=False,\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking dataset and dataloader is okay...\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "print(\"checking dataset and dataloader is okay...\")\n",
    "\n",
    "try:\n",
    "    _ = next(iter(train_loader))\n",
    "    print(\"ok\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"dataset or dataloader is not ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "--------------------------------------------------\n",
      "Epoch 0/50\n",
      "Train: 100%|██████████| 125/125 [00:58<00:00,  2.14it/s, loss - 0.6675, acc - 0.6158, F1 - 0.5790, Precision - 0.5756, Recall - 0.6158, AUC - 0.5268]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:09<00:00,  1.75it/s, loss - 0.6793, acc - 0.5440, F1 - 0.5430, Precision - 0.5420, Recall - 0.5440, AUC - 0.5098]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch0.pth\n",
      "--------------------------------------------------\n",
      "Epoch 1/50\n",
      "Train: 100%|██████████| 125/125 [00:55<00:00,  2.25it/s, loss - 0.6400, acc - 0.6419, F1 - 0.6082, Precision - 0.6109, Recall - 0.6419, AUC - 0.5560]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.87it/s, loss - 0.6605, acc - 0.6200, F1 - 0.5544, Precision - 0.5761, Recall - 0.6200, AUC - 0.5243]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch1.pth\n",
      "--------------------------------------------------\n",
      "Epoch 2/50\n",
      "Train: 100%|██████████| 125/125 [00:56<00:00,  2.20it/s, loss - 0.6074, acc - 0.6836, F1 - 0.6593, Precision - 0.6663, Recall - 0.6836, AUC - 0.6082]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.83it/s, loss - 0.6478, acc - 0.6480, F1 - 0.6517, Precision - 0.6583, Recall - 0.6480, AUC - 0.6365]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch2.pth\n",
      "--------------------------------------------------\n",
      "Epoch 3/50\n",
      "Train: 100%|██████████| 125/125 [00:56<00:00,  2.22it/s, loss - 0.5686, acc - 0.7152, F1 - 0.7009, Precision - 0.7045, Recall - 0.7152, AUC - 0.6546]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.85it/s, loss - 0.5791, acc - 0.7360, F1 - 0.7208, Precision - 0.7335, Recall - 0.7360, AUC - 0.6824]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch3.pth\n",
      "--------------------------------------------------\n",
      "Epoch 4/50\n",
      "Train: 100%|██████████| 125/125 [00:56<00:00,  2.20it/s, loss - 0.5405, acc - 0.7383, F1 - 0.7270, Precision - 0.7306, Recall - 0.7383, AUC - 0.6834]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.92it/s, loss - 0.5618, acc - 0.7360, F1 - 0.7309, Precision - 0.7307, Recall - 0.7360, AUC - 0.7021]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch4.pth\n",
      "--------------------------------------------------\n",
      "Epoch 5/50\n",
      "Train: 100%|██████████| 125/125 [00:55<00:00,  2.25it/s, loss - 0.5184, acc - 0.7554, F1 - 0.7457, Precision - 0.7494, Recall - 0.7554, AUC - 0.7040]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.86it/s, loss - 0.6015, acc - 0.7000, F1 - 0.7050, Precision - 0.7313, Recall - 0.7000, AUC - 0.7129]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch5.pth\n",
      "--------------------------------------------------\n",
      "Epoch 6/50\n",
      "Train: 100%|██████████| 125/125 [00:55<00:00,  2.27it/s, loss - 0.5184, acc - 0.7534, F1 - 0.7470, Precision - 0.7472, Recall - 0.7534, AUC - 0.7098]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.82it/s, loss - 0.5903, acc - 0.6960, F1 - 0.6535, Precision - 0.7049, Recall - 0.6960, AUC - 0.6133]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch6.pth\n",
      "--------------------------------------------------\n",
      "Epoch 7/50\n",
      "Train: 100%|██████████| 125/125 [00:53<00:00,  2.33it/s, loss - 0.5027, acc - 0.7745, F1 - 0.7668, Precision - 0.7700, Recall - 0.7745, AUC - 0.7281]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.97it/s, loss - 0.5363, acc - 0.7800, F1 - 0.7811, Precision - 0.7828, Recall - 0.7800, AUC - 0.7700]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch7.pth\n",
      "--------------------------------------------------\n",
      "Epoch 8/50\n",
      "Train:   1%|          | 1/125 [00:01<02:49,  1.36s/it, loss - 0.4638, acc - 0.8125, F1 - 0.8966, Precision - 1.0000, Recall - 0.8125, AUC - -1.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmsai2/.conda/envs/openmmlab/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 125/125 [00:55<00:00,  2.27it/s, loss - 0.4990, acc - 0.7745, F1 - 0.7677, Precision - 0.7698, Recall - 0.7745, AUC - 0.7303]\n",
      "{'precision': 0, 'recall': 0, 'auc': 1}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.84it/s, loss - 0.6183, acc - 0.7080, F1 - 0.7128, Precision - 0.7339, Recall - 0.7080, AUC - 0.7171]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch8.pth\n",
      "--------------------------------------------------\n",
      "Epoch 9/50\n",
      "Train: 100%|██████████| 125/125 [00:56<00:00,  2.22it/s, loss - 0.4743, acc - 0.7885, F1 - 0.7836, Precision - 0.7846, Recall - 0.7885, AUC - 0.7499]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.95it/s, loss - 0.5374, acc - 0.7480, F1 - 0.7366, Precision - 0.7450, Recall - 0.7480, AUC - 0.7007]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch9.pth\n",
      "--------------------------------------------------\n",
      "Epoch 10/50\n",
      "Train: 100%|██████████| 125/125 [00:53<00:00,  2.35it/s, loss - 0.4515, acc - 0.7961, F1 - 0.7915, Precision - 0.7925, Recall - 0.7961, AUC - 0.7589]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.90it/s, loss - 0.5305, acc - 0.7480, F1 - 0.7465, Precision - 0.7456, Recall - 0.7480, AUC - 0.7248]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch10.pth\n",
      "--------------------------------------------------\n",
      "Epoch 11/50\n",
      "Train: 100%|██████████| 125/125 [00:54<00:00,  2.31it/s, loss - 0.4521, acc - 0.7936, F1 - 0.7891, Precision - 0.7899, Recall - 0.7936, AUC - 0.7567]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.92it/s, loss - 0.5158, acc - 0.7720, F1 - 0.7717, Precision - 0.7715, Recall - 0.7720, AUC - 0.7549]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch11.pth\n",
      "--------------------------------------------------\n",
      "Epoch 12/50\n",
      "Train: 100%|██████████| 125/125 [00:51<00:00,  2.41it/s, loss - 0.4448, acc - 0.8036, F1 - 0.8003, Precision - 0.8004, Recall - 0.8036, AUC - 0.7709]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.93it/s, loss - 0.5371, acc - 0.7920, F1 - 0.7873, Precision - 0.7896, Recall - 0.7920, AUC - 0.7599]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch12.pth\n",
      "--------------------------------------------------\n",
      "Epoch 13/50\n",
      "Train: 100%|██████████| 125/125 [00:53<00:00,  2.35it/s, loss - 0.4186, acc - 0.8267, F1 - 0.8243, Precision - 0.8244, Recall - 0.8267, AUC - 0.7984]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.87it/s, loss - 0.5462, acc - 0.7360, F1 - 0.7258, Precision - 0.7307, Recall - 0.7360, AUC - 0.6912]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch13.pth\n",
      "--------------------------------------------------\n",
      "Epoch 14/50\n",
      "Train: 100%|██████████| 125/125 [00:56<00:00,  2.23it/s, loss - 0.4224, acc - 0.8137, F1 - 0.8112, Precision - 0.8110, Recall - 0.8137, AUC - 0.7848]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.92it/s, loss - 0.4717, acc - 0.7960, F1 - 0.7942, Precision - 0.7938, Recall - 0.7960, AUC - 0.7740]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch14.pth\n",
      "--------------------------------------------------\n",
      "Epoch 15/50\n",
      "Train: 100%|██████████| 125/125 [00:54<00:00,  2.29it/s, loss - 0.4146, acc - 0.8167, F1 - 0.8145, Precision - 0.8142, Recall - 0.8167, AUC - 0.7890]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.90it/s, loss - 0.5556, acc - 0.7280, F1 - 0.7286, Precision - 0.7292, Recall - 0.7280, AUC - 0.7111]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch15.pth\n",
      "--------------------------------------------------\n",
      "Epoch 16/50\n",
      "Train: 100%|██████████| 125/125 [00:54<00:00,  2.28it/s, loss - 0.4117, acc - 0.8257, F1 - 0.8231, Precision - 0.8234, Recall - 0.8257, AUC - 0.7963]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.89it/s, loss - 0.4660, acc - 0.8120, F1 - 0.8087, Precision - 0.8101, Recall - 0.8120, AUC - 0.7846]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch16.pth\n",
      "--------------------------------------------------\n",
      "Epoch 17/50\n",
      "Train: 100%|██████████| 125/125 [00:56<00:00,  2.21it/s, loss - 0.3934, acc - 0.8242, F1 - 0.8220, Precision - 0.8219, Recall - 0.8242, AUC - 0.7968]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.82it/s, loss - 0.5064, acc - 0.7960, F1 - 0.7970, Precision - 0.7987, Recall - 0.7960, AUC - 0.7872]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch17.pth\n",
      "--------------------------------------------------\n",
      "Epoch 18/50\n",
      "Train: 100%|██████████| 125/125 [00:56<00:00,  2.20it/s, loss - 0.3762, acc - 0.8332, F1 - 0.8321, Precision - 0.8316, Recall - 0.8332, AUC - 0.8112]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.88it/s, loss - 0.5478, acc - 0.7560, F1 - 0.7539, Precision - 0.7530, Recall - 0.7560, AUC - 0.7312]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch18.pth\n",
      "--------------------------------------------------\n",
      "Epoch 19/50\n",
      "Train: 100%|██████████| 125/125 [00:58<00:00,  2.15it/s, loss - 0.3784, acc - 0.8368, F1 - 0.8353, Precision - 0.8350, Recall - 0.8368, AUC - 0.8136]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.80it/s, loss - 0.4837, acc - 0.7760, F1 - 0.7724, Precision - 0.7726, Recall - 0.7760, AUC - 0.7471]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch19.pth\n",
      "--------------------------------------------------\n",
      "Epoch 20/50\n",
      "Train: 100%|██████████| 125/125 [00:56<00:00,  2.22it/s, loss - 0.3613, acc - 0.8453, F1 - 0.8442, Precision - 0.8438, Recall - 0.8453, AUC - 0.8240]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.85it/s, loss - 0.4405, acc - 0.8160, F1 - 0.8124, Precision - 0.8145, Recall - 0.8160, AUC - 0.7878]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch20.pth\n",
      "--------------------------------------------------\n",
      "Epoch 21/50\n",
      "Train: 100%|██████████| 125/125 [00:56<00:00,  2.22it/s, loss - 0.3613, acc - 0.8408, F1 - 0.8396, Precision - 0.8392, Recall - 0.8408, AUC - 0.8189]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.85it/s, loss - 0.5027, acc - 0.8000, F1 - 0.7991, Precision - 0.7985, Recall - 0.8000, AUC - 0.7816]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch21.pth\n",
      "--------------------------------------------------\n",
      "Epoch 22/50\n",
      "Train: 100%|██████████| 125/125 [00:53<00:00,  2.34it/s, loss - 0.3471, acc - 0.8594, F1 - 0.8584, Precision - 0.8582, Recall - 0.8594, AUC - 0.8401]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.91it/s, loss - 0.5437, acc - 0.7840, F1 - 0.7840, Precision - 0.7840, Recall - 0.7840, AUC - 0.7689]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch22.pth\n",
      "--------------------------------------------------\n",
      "Epoch 23/50\n",
      "Train: 100%|██████████| 125/125 [00:53<00:00,  2.32it/s, loss - 0.3275, acc - 0.8604, F1 - 0.8598, Precision - 0.8595, Recall - 0.8604, AUC - 0.8434]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.93it/s, loss - 0.5092, acc - 0.7920, F1 - 0.7899, Precision - 0.7895, Recall - 0.7920, AUC - 0.7686]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch23.pth\n",
      "--------------------------------------------------\n",
      "Epoch 24/50\n",
      "Train: 100%|██████████| 125/125 [00:56<00:00,  2.23it/s, loss - 0.3115, acc - 0.8654, F1 - 0.8648, Precision - 0.8645, Recall - 0.8654, AUC - 0.8486]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.89it/s, loss - 0.5774, acc - 0.7680, F1 - 0.7710, Precision - 0.7802, Recall - 0.7680, AUC - 0.7693]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch24.pth\n",
      "--------------------------------------------------\n",
      "Epoch 25/50\n",
      "Train: 100%|██████████| 125/125 [00:52<00:00,  2.38it/s, loss - 0.3114, acc - 0.8659, F1 - 0.8655, Precision - 0.8652, Recall - 0.8659, AUC - 0.8503]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.93it/s, loss - 0.5598, acc - 0.7600, F1 - 0.7625, Precision - 0.7685, Recall - 0.7600, AUC - 0.7563]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch25.pth\n",
      "--------------------------------------------------\n",
      "Epoch 26/50\n",
      "Train: 100%|██████████| 125/125 [00:56<00:00,  2.21it/s, loss - 0.3166, acc - 0.8654, F1 - 0.8646, Precision - 0.8644, Recall - 0.8654, AUC - 0.8473]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.87it/s, loss - 0.5310, acc - 0.8200, F1 - 0.8194, Precision - 0.8190, Recall - 0.8200, AUC - 0.8041]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch26.pth\n",
      "--------------------------------------------------\n",
      "Epoch 27/50\n",
      "Train: 100%|██████████| 125/125 [00:54<00:00,  2.29it/s, loss - 0.2873, acc - 0.8774, F1 - 0.8774, Precision - 0.8774, Recall - 0.8774, AUC - 0.8656]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.95it/s, loss - 0.6318, acc - 0.8040, F1 - 0.8038, Precision - 0.8036, Recall - 0.8040, AUC - 0.7892]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch27.pth\n",
      "--------------------------------------------------\n",
      "Epoch 28/50\n",
      "Train: 100%|██████████| 125/125 [00:54<00:00,  2.29it/s, loss - 0.2844, acc - 0.8820, F1 - 0.8819, Precision - 0.8819, Recall - 0.8820, AUC - 0.8704]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.90it/s, loss - 0.5325, acc - 0.8160, F1 - 0.8097, Precision - 0.8178, Recall - 0.8160, AUC - 0.7790]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch28.pth\n",
      "--------------------------------------------------\n",
      "Epoch 29/50\n",
      "Train: 100%|██████████| 125/125 [00:55<00:00,  2.25it/s, loss - 0.2779, acc - 0.8935, F1 - 0.8931, Precision - 0.8930, Recall - 0.8935, AUC - 0.8800]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.82it/s, loss - 0.5708, acc - 0.7960, F1 - 0.7948, Precision - 0.7942, Recall - 0.7960, AUC - 0.7762]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch29.pth\n",
      "--------------------------------------------------\n",
      "Epoch 30/50\n",
      "Train: 100%|██████████| 125/125 [00:53<00:00,  2.34it/s, loss - 0.2587, acc - 0.8875, F1 - 0.8873, Precision - 0.8872, Recall - 0.8875, AUC - 0.8757]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.94it/s, loss - 0.5468, acc - 0.8040, F1 - 0.8038, Precision - 0.8036, Recall - 0.8040, AUC - 0.7892]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch30.pth\n",
      "--------------------------------------------------\n",
      "Epoch 31/50\n",
      "Train: 100%|██████████| 125/125 [00:54<00:00,  2.30it/s, loss - 0.2224, acc - 0.9111, F1 - 0.9110, Precision - 0.9109, Recall - 0.9111, AUC - 0.9013]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.92it/s, loss - 0.6110, acc - 0.7560, F1 - 0.7600, Precision - 0.7815, Recall - 0.7560, AUC - 0.7685]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch31.pth\n",
      "--------------------------------------------------\n",
      "Epoch 32/50\n",
      "Train: 100%|██████████| 125/125 [00:53<00:00,  2.33it/s, loss - 0.2466, acc - 0.9021, F1 - 0.9022, Precision - 0.9023, Recall - 0.9021, AUC - 0.8940]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.90it/s, loss - 0.4547, acc - 0.8160, F1 - 0.8171, Precision - 0.8191, Recall - 0.8160, AUC - 0.8097]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch32.pth\n",
      "--------------------------------------------------\n",
      "Epoch 33/50\n",
      "Train: 100%|██████████| 125/125 [00:52<00:00,  2.36it/s, loss - 0.2222, acc - 0.9141, F1 - 0.9141, Precision - 0.9141, Recall - 0.9141, AUC - 0.9062]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.89it/s, loss - 0.5563, acc - 0.8000, F1 - 0.7961, Precision - 0.7977, Recall - 0.8000, AUC - 0.7706]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch33.pth\n",
      "--------------------------------------------------\n",
      "Epoch 34/50\n",
      "Train: 100%|██████████| 125/125 [00:53<00:00,  2.33it/s, loss - 0.2031, acc - 0.9111, F1 - 0.9113, Precision - 0.9116, Recall - 0.9111, AUC - 0.9052]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.81it/s, loss - 0.6767, acc - 0.7760, F1 - 0.7731, Precision - 0.7728, Recall - 0.7760, AUC - 0.7493]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch34.pth\n",
      "--------------------------------------------------\n",
      "Epoch 35/50\n",
      "Train: 100%|██████████| 125/125 [00:57<00:00,  2.18it/s, loss - 0.2131, acc - 0.9161, F1 - 0.9160, Precision - 0.9160, Recall - 0.9161, AUC - 0.9071]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.89it/s, loss - 0.6603, acc - 0.7520, F1 - 0.7549, Precision - 0.7625, Recall - 0.7520, AUC - 0.7499]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch35.pth\n",
      "--------------------------------------------------\n",
      "Epoch 36/50\n",
      "Train: 100%|██████████| 125/125 [00:54<00:00,  2.31it/s, loss - 0.2110, acc - 0.9216, F1 - 0.9216, Precision - 0.9216, Recall - 0.9216, AUC - 0.9143]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.94it/s, loss - 0.6232, acc - 0.7800, F1 - 0.7802, Precision - 0.7805, Recall - 0.7800, AUC - 0.7657]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch36.pth\n",
      "--------------------------------------------------\n",
      "Epoch 37/50\n",
      "Train: 100%|██████████| 125/125 [00:54<00:00,  2.31it/s, loss - 0.1862, acc - 0.9297, F1 - 0.9298, Precision - 0.9298, Recall - 0.9297, AUC - 0.9241]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.92it/s, loss - 0.7496, acc - 0.7840, F1 - 0.7775, Precision - 0.7821, Recall - 0.7840, AUC - 0.7469]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch37.pth\n",
      "--------------------------------------------------\n",
      "Epoch 38/50\n",
      "Train: 100%|██████████| 125/125 [00:55<00:00,  2.24it/s, loss - 0.1798, acc - 0.9272, F1 - 0.9273, Precision - 0.9276, Recall - 0.9272, AUC - 0.9224]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.91it/s, loss - 0.6799, acc - 0.7760, F1 - 0.7744, Precision - 0.7737, Recall - 0.7760, AUC - 0.7537]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch38.pth\n",
      "--------------------------------------------------\n",
      "Epoch 39/50\n",
      "Train: 100%|██████████| 125/125 [00:53<00:00,  2.35it/s, loss - 0.1765, acc - 0.9282, F1 - 0.9282, Precision - 0.9282, Recall - 0.9282, AUC - 0.9213]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.87it/s, loss - 0.8269, acc - 0.7520, F1 - 0.7561, Precision - 0.7879, Recall - 0.7520, AUC - 0.7719]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch39.pth\n",
      "--------------------------------------------------\n",
      "Epoch 40/50\n",
      "Train: 100%|██████████| 125/125 [00:55<00:00,  2.26it/s, loss - 0.1713, acc - 0.9327, F1 - 0.9328, Precision - 0.9331, Recall - 0.9327, AUC - 0.9286]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.83it/s, loss - 0.6407, acc - 0.8160, F1 - 0.8171, Precision - 0.8191, Recall - 0.8160, AUC - 0.8097]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch40.pth\n",
      "--------------------------------------------------\n",
      "Epoch 41/50\n",
      "Train: 100%|██████████| 125/125 [00:55<00:00,  2.26it/s, loss - 0.1666, acc - 0.9317, F1 - 0.9318, Precision - 0.9318, Recall - 0.9317, AUC - 0.9263]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.84it/s, loss - 0.7074, acc - 0.7640, F1 - 0.7590, Precision - 0.7600, Recall - 0.7640, AUC - 0.7310]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch41.pth\n",
      "--------------------------------------------------\n",
      "Epoch 42/50\n",
      "Train: 100%|██████████| 125/125 [00:54<00:00,  2.28it/s, loss - 0.1535, acc - 0.9427, F1 - 0.9427, Precision - 0.9426, Recall - 0.9427, AUC - 0.9364]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.88it/s, loss - 0.8245, acc - 0.7680, F1 - 0.7657, Precision - 0.7650, Recall - 0.7680, AUC - 0.7430]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch42.pth\n",
      "--------------------------------------------------\n",
      "Epoch 43/50\n",
      "Train: 100%|██████████| 125/125 [00:56<00:00,  2.22it/s, loss - 0.1287, acc - 0.9508, F1 - 0.9508, Precision - 0.9508, Recall - 0.9508, AUC - 0.9468]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.89it/s, loss - 0.7199, acc - 0.8040, F1 - 0.8023, Precision - 0.8019, Recall - 0.8040, AUC - 0.7826]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch43.pth\n",
      "--------------------------------------------------\n",
      "Epoch 44/50\n",
      "Train: 100%|██████████| 125/125 [00:54<00:00,  2.28it/s, loss - 0.1389, acc - 0.9463, F1 - 0.9463, Precision - 0.9463, Recall - 0.9463, AUC - 0.9417]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.92it/s, loss - 0.6338, acc - 0.8040, F1 - 0.8023, Precision - 0.8019, Recall - 0.8040, AUC - 0.7826]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch44.pth\n",
      "--------------------------------------------------\n",
      "Epoch 45/50\n",
      "Train: 100%|██████████| 125/125 [00:55<00:00,  2.26it/s, loss - 0.1246, acc - 0.9568, F1 - 0.9568, Precision - 0.9568, Recall - 0.9568, AUC - 0.9524]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.88it/s, loss - 0.7503, acc - 0.7960, F1 - 0.7930, Precision - 0.7934, Recall - 0.7960, AUC - 0.7696]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "pth file saved at /home/dmsai2/mmdetection/work_dir/classification/weights/2024-06-19_05_43/xception_epoch45.pth\n",
      "--------------------------------------------------\n",
      "Epoch 46/50\n",
      "Train: 100%|██████████| 125/125 [00:59<00:00,  2.10it/s, loss - 0.1194, acc - 0.9573, F1 - 0.9573, Precision - 0.9572, Recall - 0.9573, AUC - 0.9522]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n",
      "Valid: 100%|██████████| 16/16 [00:08<00:00,  1.82it/s, loss - 0.7908, acc - 0.7200, F1 - 0.7246, Precision - 0.7471, Recall - 0.7200, AUC - 0.7310]\n",
      "{'precision': 0, 'recall': 0, 'auc': 0}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:595] . unexpected pos 231007296 vs 231007184",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/openmmlab/lib/python3.8/site-packages/torch/serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 628\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/openmmlab/lib/python3.8/site-packages/torch/serialization.py:862\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    861\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 862\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:764] . PytorchStreamWriter failed writing file data/732: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m step \u001b[38;5;241m=\u001b[39m train(train_loader, model, criterion, optimizer, epoch, train_writer, step)\n\u001b[1;32m      8\u001b[0m acc \u001b[38;5;241m=\u001b[39m validate(valid_loader, model, criterion, epoch, test_writer)\n\u001b[0;32m---> 10\u001b[0m \u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_acc1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43macc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mis_best\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_epoch\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m, in \u001b[0;36msave_checkpoint\u001b[0;34m(state, is_best, filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_checkpoint\u001b[39m(state, is_best, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint.pth.tar\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpth file saved at \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m filename)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_best:\n",
      "File \u001b[0;32m~/.conda/envs/openmmlab/lib/python3.8/site-packages/torch/serialization.py:629\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    628\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[0;32m--> 629\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n",
      "File \u001b[0;32m~/.conda/envs/openmmlab/lib/python3.8/site-packages/torch/serialization.py:475\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 475\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:595] . unexpected pos 231007296 vs 231007184"
     ]
    }
   ],
   "source": [
    "print(\"start training...\")\n",
    "step = 0\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    print('-' * 50)\n",
    "    print('Epoch {}/{}'.format(epoch, args.num_epochs))\n",
    "    step = train(train_loader, model, criterion, optimizer, epoch, train_writer, step)\n",
    "    acc = validate(valid_loader, model, criterion, epoch, test_writer)\n",
    "\n",
    "    save_checkpoint(state={'epoch': epoch,\n",
    "                           'state_dict': model.state_dict(),\n",
    "                           'best_acc1': acc,\n",
    "                           'optimizer': optimizer.state_dict(),},\n",
    "                        is_best=False,\n",
    "                        filename=args.save_fn + f\"_epoch{epoch}.pth\",\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, all_labels, all_preds, f1 = validate(valid_loader, model, criterion, 0, test_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

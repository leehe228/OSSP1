{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image = \"./train/img/\"\n",
    "train_ann = \"./train/ann/\"\n",
    "val_image = \"./valid/img/\"\n",
    "val_ann = \"./valid/ann/\"  \n",
    "\n",
    "test_image = \"./test/img/\"\n",
    "test_ann = \"./test/ann/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991 1991 254 254\n"
     ]
    }
   ],
   "source": [
    "train_image_list = os.listdir(train_image)\n",
    "train_ann_list = os.listdir(train_ann)\n",
    "\n",
    "val_image_list = os.listdir(val_image)\n",
    "val_ann_list = os.listdir(val_ann)\n",
    "\n",
    "print(len(train_image_list), len(train_ann_list), len(val_image_list), len(val_ann_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 250\n"
     ]
    }
   ],
   "source": [
    "test_image_list = os.listdir(test_image)\n",
    "test_ann_list = os.listdir(test_ann)\n",
    "\n",
    "print(len(test_image_list), len(test_ann_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11398.jpg', '12660.jpg', '11488.jpg', '11154.jpg', '10105.jpg']\n",
      "['10435.json', '11631.json', '11781.json', '13126.json', '12743.json']\n",
      "['2410.jpg', '1659.jpg', '1066.jpg', '1941.jpg', '1620.jpg']\n",
      "['1049.json', '251.json', '1153.json', '1009.json', '1694.json']\n"
     ]
    }
   ],
   "source": [
    "print(train_image_list[:5])\n",
    "print(train_ann_list[:5])\n",
    "print(val_image_list[:5])\n",
    "print(val_ann_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 254/254 [00:00<00:00, 104354.32it/s]\n",
      "100%|██████████| 254/254 [00:00<00:00, 119313.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# rename image and ann\n",
    "# 3449_jpg.rf.f0cbd3e5b418f05bfdab2e4bf556086e.jpg -> 3349.jpg\n",
    "\n",
    "for i in tqdm(val_image_list):\n",
    "    os.rename(val_image+i, val_image+i.split(\"_\")[0]+\".jpg\")\n",
    "    \n",
    "# rename ann json\n",
    "# 1_jpg.rf.a332191a2b0b318c103a508c515931c1.jpg.json -> 1.json\n",
    "for i in tqdm(val_ann_list):\n",
    "    os.rename(val_ann+i, val_ann+i.split(\"_\")[0]+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1991/1991 [00:00<00:00, 109048.95it/s]\n",
      "100%|██████████| 1991/1991 [00:00<00:00, 125908.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# rename image and ann\n",
    "# 1.jpg -> 10001.jpg, 1.json -> 10001.json\n",
    "for i in tqdm(train_image_list):\n",
    "    os.rename(train_image+i, train_image+str(int(i.split(\".\")[0])+10000)+\".jpg\")\n",
    "    \n",
    "for i in tqdm(train_ann_list):\n",
    "    os.rename(train_ann+i, train_ann+str(int(i.split(\".\")[0])+10000)+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 106747.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# for i in tqdm(test_image_list):\n",
    "#     os.rename(test_image+i, test_image+str(int(i.split(\"_\")[0])+20000)+\".jpg\")\n",
    "\n",
    "for i in tqdm(test_ann_list):\n",
    "    os.rename(test_ann+i, test_ann+str(int(i.split(\"_\")[0])+20000)+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 501231.36it/s]\n"
     ]
    }
   ],
   "source": [
    "error_list = []\n",
    "\n",
    "# check image and ann matching\n",
    "for i in tqdm(test_image_list):\n",
    "    if i.split(\".\")[0] + \".json\" not in test_ann_list:\n",
    "        error_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat ann json file to COCO dataset format\n",
    "def reformat_ann(ann_path, image_path, save_path):\n",
    "    with open(ann_path, 'r') as f:\n",
    "        ann = json.load(f)\n",
    "    with open(image_path, 'r') as f:\n",
    "        image = json.load(f)\n",
    "    \n",
    "    new_ann = {}\n",
    "    new_ann['info'] = ann['info']\n",
    "    new_ann['licenses'] = ann['licenses']\n",
    "    new_ann['images'] = []\n",
    "    new_ann['annotations'] = []\n",
    "    new_ann['categories'] = ann['categories']\n",
    "    \n",
    "    for i in range(len(ann['images'])):\n",
    "        new_ann['images'].append(ann['images'][i])\n",
    "        new_ann['annotations'].append(ann['annotations'][i])\n",
    "    \n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(new_ann, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_to_coco(data_dir, output_file):\n",
    "    images = []\n",
    "    annotations = []\n",
    "    categories = [{\"id\": 1, \"name\": \"tooth\", \"supercategory\": \"none\"}]\n",
    "    \n",
    "    category_id = 1  # Since all categories are \"Tooth\"\n",
    "    annotation_id = 1\n",
    "    \n",
    "    img_dir = os.path.join(data_dir, 'img')\n",
    "    ann_dir = os.path.join(data_dir, 'ann')\n",
    "    \n",
    "    img_files = sorted(glob.glob(os.path.join(img_dir, '*.jpg')))\n",
    "    \n",
    "    for img_id, img_file in enumerate(tqdm(img_files), 1):\n",
    "        img_name = os.path.basename(img_file)\n",
    "        ann_file = os.path.join(ann_dir, os.path.splitext(img_name)[0] + '.json')\n",
    "\n",
    "        if not os.path.exists(ann_file):\n",
    "            continue\n",
    "        \n",
    "        with open(ann_file, 'r') as f:\n",
    "            ann_data = json.load(f)\n",
    "        \n",
    "        # Add image information\n",
    "        images.append({\n",
    "            \"id\": img_id,\n",
    "            \"file_name\": img_name,\n",
    "            \"height\": ann_data['size']['height'],\n",
    "            \"width\": ann_data['size']['width']\n",
    "        })\n",
    "        \n",
    "        # Add annotations\n",
    "        for obj in ann_data['objects']:\n",
    "            points = obj['points']['exterior']\n",
    "            # Convert polygon to COCO format (x, y, width, height)\n",
    "            x_coords = [p[0] for p in points]\n",
    "            y_coords = [p[1] for p in points]\n",
    "            min_x, min_y = min(x_coords), min(y_coords)\n",
    "            max_x, max_y = max(x_coords), max(y_coords)\n",
    "            width, height = max_x - min_x, max_y - min_y\n",
    "            \n",
    "            annotations.append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"segmentation\": [list(sum(points, []))],  # Flatten the list of points\n",
    "                \"area\": width * height,\n",
    "                \"bbox\": [min_x, min_y, width, height],\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            annotation_id += 1\n",
    "    \n",
    "    coco_format = {\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "        \"categories\": categories\n",
    "    }\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(coco_format, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1991/1991 [00:00<00:00, 3321.81it/s]\n"
     ]
    }
   ],
   "source": [
    "convert_to_coco('./train/', 'train_annotations.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 254/254 [00:00<00:00, 4019.78it/s]\n"
     ]
    }
   ],
   "source": [
    "convert_to_coco('./valid/', 'valid_annotations.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 1808.54it/s]\n"
     ]
    }
   ],
   "source": [
    "convert_to_coco('./test/', 'test_annotations.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
